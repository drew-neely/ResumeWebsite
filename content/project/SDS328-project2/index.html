---
title: "Analysis of Computer and Human Generated Random Number Sequences"
author: "Drew Neely (dln696)"
date: "2019-11-26T00:00:00Z"
output:
  html_document: default
  pdf_document: default
summary: "A statistical analysis with the goal of predicting the creator of computer and human generated random numbers."
tags:
- Data Science
---



<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>The objective of this project is to discover features of numbers generated ‘randomly’ by a human and generated randomly by a computer that can be used to predict the creator of the ‘random’ number. To do this, I surveyed 10 people to produce 10 10-digit strings of random numbers. I also gathered some other potentially relevant information, then anonymized the data. The data collected from the 10 people are shown below.</p>
<pre class="r"><code>numbers_wide &lt;- read.csv(&quot;random_human_numbers.csv&quot;)
head(numbers_wide)</code></pre>
<pre><code>##   Name Human Gender Age Political.Party Studied.Science      Seq.1
## 1    A   Yes Female  21        Democrat             Yes 2789956111
## 2    B   Yes Female  21        Democrat             Yes 4985720117
## 3    C   Yes   Male  49      Republican             Yes 6221014971
## 4    D   Yes Female  20        Democrat             Yes 2438509376
## 5    E   Yes   Male  21        Democrat             Yes 1865318579
## 6    F   Yes Female  20        Democrat              No 1547869073
##        Seq.2      Seq.3      Seq.4      Seq.5      Seq.6      Seq.7
## 1 1366759287 7635557721 9422606787 5705422787 2967210348 9756780291
## 2 6590031429 6404098972 2314564259 8712385119 1203343473 5645786902
## 3 4179819803 2317891995 1908913123 5079181795 6481046253 2957108139
## 4 1845910348 2071394705 1893526313 5724680516 2739152704 1637923840
## 5 9987412378 1721834778 1321865554 1980874319 8700918782 1384794786
## 6 5841290825 4613798534 6871045978 2103776154 9626598215 4065982138
##        Seq.8      Seq.9     Seq.10
## 1 5497651238 4591268881 4295565751
## 2 1985763498 7050413427 6802232476
## 3 1221316015 4620378902 8763219721
## 4 6932674084 6905183951 8390649135
## 5 5431821870 8479871231 4798218976
## 6 7596053467 9283153469 8218083509</code></pre>
<p>I then used <code>pivot_longer</code> to make this data long.</p>
<pre class="r"><code>numbers &lt;- numbers_wide %&gt;% 
  pivot_longer(starts_with(&quot;Seq&quot;), values_to = &quot;Number&quot;, names_to = &quot;Seq&quot;, names_prefix = &quot;Seq.&quot;)
head(numbers)</code></pre>
<pre><code>## # A tibble: 6 x 8
##   Name  Human Gender   Age Political.Party Studied.Science Seq       Number
##   &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;  &lt;int&gt; &lt;fct&gt;           &lt;fct&gt;           &lt;chr&gt;      &lt;dbl&gt;
## 1 A     Yes   Female    21 Democrat        Yes             1     2789956111
## 2 A     Yes   Female    21 Democrat        Yes             2     1366759287
## 3 A     Yes   Female    21 Democrat        Yes             3     7635557721
## 4 A     Yes   Female    21 Democrat        Yes             4     9422606787
## 5 A     Yes   Female    21 Democrat        Yes             5     5705422787
## 6 A     Yes   Female    21 Democrat        Yes             6     2967210348</code></pre>
<p>I then added 100 computer-generated random 10-digit strings onto the end of the 100 human generated random strings.</p>
<pre class="r"><code>set.seed(666)
for(i in 1:100) {
  num &lt;- paste(sample(0:9, 10, replace = TRUE), collapse = &#39;&#39;)
  df &lt;- data.frame(NA, &quot;No&quot;, NA, NA, NA, NA, NA, num)
  names(df) &lt;- names(numbers)
  numbers &lt;- rbind(numbers, df)
}</code></pre>
<p>I then wrote the dataframe to a csv file and switched over to python to extract the features.</p>
<pre class="r"><code>write.csv(numbers, &quot;random_numbers.csv&quot;)</code></pre>
<p>I generated several features for each observation and exported a new csv file using the following python code.</p>
<pre class="python"><code>import csv
import statistics 

entries = []

# import csv
csv.register_dialect(&#39;myDialect&#39;,delimiter = &#39;,&#39;,quoting=csv.QUOTE_ALL,skipinitialspace=True)
with open(&#39;random_numbers.csv&#39;, &#39;r&#39;) as f:
    reader = csv.DictReader(f, dialect=&#39;myDialect&#39;)
    for row in reader:
        entries += [dict(row)]

######################################################################

def addFeatures(entry) :
    seq = entry[&#39;Number&#39;]
    while len(seq) &lt; 10 :
        seq = &#39;0&#39; + seq
    
    # digit frequency
    max = 0
    for d in range(0,10) :
        count = 0
        for i in range(0, 10) :
            if str(d) == seq[i] :
                count += 1
        entry[&#39;count.&#39;+str(d)] = count
        if count &gt; max :
            max = count
    entry[&#39;count.max.duplicates&#39;] = max

    # number of consecutive numbers
    count = 0
    for i in range(1, 10) :
        if int(seq[i]) == int(seq[i-1]) - 1 or int(seq[i]) == int(seq[i-1]) + 1 :
            count += 1
    entry[&#39;num.consecutive&#39;] = count

    # derivative
    diff = []
    for i in range(1, 10) :
        diff += [int(seq[i]) - int(seq[i-1])]
    deriv = statistics.mean(diff)
    entry[&#39;derivative&#39;] = int(deriv * 100) / 100
    entry[&#39;growth.direction&#39;] = &#39;Increase&#39; if deriv &gt; 0 else (&#39;Decrease&#39; if deriv &lt; 0 else &#39;Zero&#39;)

    # mean
    sum = 0
    for i in range(0, 10) :
        sum += int(seq[i])
    entry[&#39;mean&#39;] = sum / 10

    # standard deviation
    entry[&#39;std.dev&#39;] = int(statistics.stdev([int(seq[i]) for i in range(0, 10)]) * 1000) / 1000
    # even and odd
    entry[&#39;num.odd&#39;] = len([int(seq[i]) for i in range(0, 10) if int(seq[i]) % 2 == 1])
    entry[&#39;num.even&#39;] = len([int(seq[i]) for i in range(0, 10) if int(seq[i]) % 2 == 0])
    
    return entry

######################################################################

# Generate features
for i in range(0, len(entries)) :
    entries[i] = addFeatures(entries[i])

# Export csv
with open(&#39;random_numbers_with_features.csv&#39;, mode=&#39;w&#39;) as csv_file:
    writer = csv.DictWriter(csv_file, fieldnames=entries[0].keys())

    writer.writeheader()
    for i in range(0, len(entries)) :
        writer.writerow(entries[i])</code></pre>
<p>The features generated include the following : frequency of each digit, the highest frequency of any digit, the number of consecutive digits, the derivative of the sequence, the sign of the derivative (categorical), the mean of the sequence, the standard deviation of the sequence, and the number of even and odd digits in the sequence.</p>
<p>Now this csv can be re-imported into R.</p>
<pre class="r"><code>features &lt;- read.csv(&#39;random_numbers_with_features.csv&#39;)</code></pre>
<p>Then I can add a binary variable ‘Is.Human’ that is 1 if the sequence was generated by a human and 0 if the sequence was generated by a machine. I can also drop the variables I will not be using for testing.</p>
<pre class="r"><code>features &lt;- features %&gt;%
  mutate(&quot;Is.Human&quot; = if_else(Human == &quot;Yes&quot;, 1, 0)) %&gt;%
  select(-Name, -Human, -Gender, -Age, -Political.Party, -Studied.Science, -Seq, -Number)</code></pre>
<p>Finally I can randomize the order of the rows.</p>
<pre class="r"><code>set.seed(666)
features &lt;- features[sample(nrow(features)), ]</code></pre>
<p>Now the features dataset is complete!</p>
<pre class="r"><code>head(features)</code></pre>
<pre><code>##     count.0 count.1 count.2 count.3 count.4 count.5 count.6 count.7
## 62        1       1       1       1       1       1       0       2
## 126       2       2       0       0       1       0       3       0
## 96        0       2       1       1       2       1       1       1
## 139       0       2       1       0       0       0       1       1
## 123       0       2       2       1       0       2       1       1
## 28        1       4       2       1       0       1       1       0
##     count.8 count.9 count.max.duplicates num.consecutive derivative
## 62        1       1                    2               3       0.11
## 126       1       1                    3               1      -0.66
## 96        1       0                    2               3       0.33
## 139       2       3                    3               1       0.11
## 123       1       0                    2               0       0.11
## 28        0       0                    4               3       0.44
##     growth.direction mean std.dev num.odd num.even Is.Human
## 62          Increase  4.6   3.098       6        4        1
## 126         Decrease  4.1   3.381       3        7        0
## 96          Increase  4.1   2.424       5        5        1
## 139         Increase  6.0   3.366       6        4        0
## 123         Increase  4.0   2.538       6        4        0
## 28          Increase  2.2   1.932       6        4        1</code></pre>
</div>
<div id="manova-test" class="section level1">
<h1>MANOVA Test</h1>
<p>Because some subsets of the data can be used to perfectly predict other features of this data (for example, if we know the frequency of the numbers 0-8 in a sequence we can easily calculate the frequency of 9s, the mean, and the standard deviation), multiple Manovas must be done with different sets of variables. It is likely that this division suggests we have more features than are likely to be helpful in creating a model to predict the data, but we will ignore this in favor of simplicity.</p>
<div id="digit-frequencies" class="section level3">
<h3>Digit Frequencies</h3>
<p>The first manova we shall run will test if the frequency of numbers in the sequence is significantly different between different sources of the sequence.</p>
<pre class="r"><code>freqMan&lt;-manova(cbind(count.0,count.1,count.2,
                      count.3,count.4,count.5,
                      count.6,count.7,count.8) ~ Is.Human,
                data=features)
summary(freqMan)</code></pre>
<pre><code>##            Df   Pillai approx F num Df den Df  Pr(&gt;F)  
## Is.Human    1 0.077249   1.7673      9    190 0.07681 .
## Residuals 198                                          
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Since this is a relatively small dataset for what we are studying and the cost of a type 1 error (finding a non-predictor to be a predictor) is likely less than that of a type 2 error, we will use a relatively high significance threshold of 0.10.</p>
<p>With a p-value of 0.07681, the frequency of the numbers in the sequence is significant. As such, we must now run a multiple Anova tests to determine which features are significant predictors.</p>
<pre class="r"><code>summary.aov(freqMan)</code></pre>
<pre><code>##  Response count.0 :
##              Df  Sum Sq Mean Sq F value Pr(&gt;F)
## Is.Human      1   0.005 0.00500  0.0067 0.9348
## Residuals   198 147.390 0.74439               
## 
##  Response count.1 :
##              Df  Sum Sq Mean Sq F value Pr(&gt;F)
## Is.Human      1   0.605 0.60500  0.8025 0.3714
## Residuals   198 149.270 0.75389               
## 
##  Response count.2 :
##              Df  Sum Sq Mean Sq F value  Pr(&gt;F)  
## Is.Human      1   2.645 2.64500  4.1449 0.04309 *
## Residuals   198 126.350 0.63813                  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
##  Response count.3 :
##              Df Sum Sq Mean Sq F value Pr(&gt;F)
## Is.Human      1   0.72  0.7200  0.8893 0.3468
## Residuals   198 160.30  0.8096               
## 
##  Response count.4 :
##              Df Sum Sq Mean Sq F value Pr(&gt;F)
## Is.Human      1   0.02 0.02000  0.0276 0.8681
## Residuals   198 143.26 0.72354               
## 
##  Response count.5 :
##              Df Sum Sq Mean Sq F value Pr(&gt;F)
## Is.Human      1   2.42 2.42000  2.6995  0.102
## Residuals   198 177.50 0.89646               
## 
##  Response count.6 :
##              Df  Sum Sq Mean Sq F value  Pr(&gt;F)  
## Is.Human      1   2.205 2.20500  3.6581 0.05724 .
## Residuals   198 119.350 0.60278                  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
##  Response count.7 :
##              Df  Sum Sq Mean Sq F value  Pr(&gt;F)  
## Is.Human      1   3.125 3.12500  4.5742 0.03368 *
## Residuals   198 135.270 0.68318                  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
##  Response count.8 :
##              Df Sum Sq Mean Sq F value Pr(&gt;F)
## Is.Human      1    0.5 0.50000  0.6471 0.4221
## Residuals   198  153.0 0.77273</code></pre>
<pre class="r"><code>summary(aov(count.9 ~ Is.Human, data = features))</code></pre>
<pre><code>##              Df Sum Sq Mean Sq F value Pr(&gt;F)
## Is.Human      1   0.04  0.0450   0.062  0.804
## Residuals   198 144.71  0.7309</code></pre>
<p>This tells us that the frequency of ‘2’, ‘6’, and ‘7’ are significantly different between computer generated random numbers and human generated random numbers.</p>
<pre class="r"><code>features %&gt;% 
  group_by(Is.Human) %&gt;% 
  summarise(&quot;count.2&quot; = mean(count.2), 
            &quot;count.6&quot; = mean(count.6), 
            &quot;count.7&quot; = mean(count.7))</code></pre>
<pre><code>## # A tibble: 2 x 4
##   Is.Human count.2 count.6 count.7
##      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
## 1        0    0.88    1.02    0.82
## 2        1    1.11    0.81    1.07</code></pre>
<p>These results show that humans use more twos and sevens and fewer sixes. However, the results regarding the frequencies of twos and seven invite suspicion since we expect to see 1 as the mean count of every number generated by a computer, but the computer generated sequence frequencies differ from 1 by more than 10% when considering twos and sevens.</p>
<p>A post-hoc t-test for the frequency of twos, sixes, and sevens is shown below (even though since there are only 2 categories in the Is.Human field this is kind of pointless)</p>
<pre class="r"><code>pairwise.t.test(features$count.2, features$Is.Human, p.adj=&quot;none&quot;)</code></pre>
<pre><code>## 
##  Pairwise comparisons using t tests with pooled SD 
## 
## data:  features$count.2 and features$Is.Human 
## 
##   0    
## 1 0.043
## 
## P value adjustment method: none</code></pre>
<pre class="r"><code>pairwise.t.test(features$count.6, features$Is.Human, p.adj=&quot;none&quot;)</code></pre>
<pre><code>## 
##  Pairwise comparisons using t tests with pooled SD 
## 
## data:  features$count.6 and features$Is.Human 
## 
##   0    
## 1 0.057
## 
## P value adjustment method: none</code></pre>
<pre class="r"><code>pairwise.t.test(features$count.7, features$Is.Human, p.adj=&quot;none&quot;)</code></pre>
<pre><code>## 
##  Pairwise comparisons using t tests with pooled SD 
## 
## data:  features$count.7 and features$Is.Human 
## 
##   0    
## 1 0.034
## 
## P value adjustment method: none</code></pre>
</div>
<div id="distribution-properties" class="section level3">
<h3>Distribution Properties</h3>
<p>Now we can test the other features of the sequence that were extracted.</p>
<pre class="r"><code>distMan&lt;-manova(cbind(count.max.duplicates, num.consecutive,
                      derivative, growth.direction, mean,
                      std.dev, num.even) ~ Is.Human,
                data=features)</code></pre>
<pre class="r"><code>summary.aov(distMan)</code></pre>
<pre><code>##  Response count.max.duplicates :
##              Df Sum Sq Mean Sq F value    Pr(&gt;F)    
## Is.Human      1   7.22  7.2200  19.199 1.909e-05 ***
## Residuals   198  74.46  0.3761                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
##  Response num.consecutive :
##              Df Sum Sq Mean Sq F value    Pr(&gt;F)    
## Is.Human      1  27.38 27.3800  15.452 0.0001169 ***
## Residuals   198 350.84  1.7719                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
##  Response derivative :
##              Df Sum Sq Mean Sq F value Pr(&gt;F)
## Is.Human      1  0.128 0.12802  0.6849 0.4089
## Residuals   198 37.008 0.18691               
## 
##  Response growth.direction :
##              Df Sum Sq Mean Sq F value Pr(&gt;F)
## Is.Human      1  0.245 0.24500  0.5294 0.4677
## Residuals   198 91.630 0.46278               
## 
##  Response mean :
##              Df  Sum Sq Mean Sq F value Pr(&gt;F)
## Is.Human      1   0.361 0.36125  0.5934  0.442
## Residuals   198 120.533 0.60875               
## 
##  Response std.dev :
##              Df Sum Sq Mean Sq F value  Pr(&gt;F)  
## Is.Human      1  0.629 0.62922  2.8836 0.09106 .
## Residuals   198 43.205 0.21821                  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
##  Response num.even :
##              Df Sum Sq Mean Sq F value Pr(&gt;F)
## Is.Human      1   0.12  0.1250  0.0544 0.8159
## Residuals   198 455.27  2.2993</code></pre>
<pre class="r"><code>summary(aov(num.odd ~ Is.Human, data = features))</code></pre>
<pre><code>##              Df Sum Sq Mean Sq F value Pr(&gt;F)
## Is.Human      1    0.1   0.125   0.054  0.816
## Residuals   198  455.3   2.299</code></pre>
<p>From this we can see that there is a very significant difference between the two groups in terms of the variables count.max.duplicate (the highest count of the digits in the sequence), and num.consecutive (the number of consecutive numbers in the sequence). There is also a less significant difference between the two groups in terms of the variable std.dev (the standard deviation of the digits in the sequence).</p>
<p>I will not include a post-hoc t-test for this set of variables since there are only two groups.</p>
</div>
<div id="multiple-comparisons-correction" class="section level3">
<h3>Multiple Comparisons Correction</h3>
<p>In this analysis, 18 null hypothesis were tested. However, the largest subset of the variables tested which together do not perfectly predict any other variable in the subset (via linear combination) is of size 14. This excludes count.9, mean, num.even, and num.odd. This we will consider this analysis to consist of 14 comparisons.</p>
<p>Since we picked a significance threshold of 0.10, the probability that any given test yields a type 1 error is 10%. Thus, the probability that there is a type 1 error across our 14 comparisons is 1 - (1 - 0.10)<sup>14</sup> = 77%. This rather high, so to correct for multiple comparisons we will use the bonferroni correction. This requires us to divide our significance level by 14 in order to keep the probability of a type 1 error occuring at 10%. The new significance level after correcting for multiple comparisons is 7.143e-3.</p>
</div>
<div id="significant-variables" class="section level3">
<h3>Significant Variables</h3>
<p>Using a corrected significance level of 7.143e-3, several variables which were before significant no longer are. The two explanatory variables which still have significant differences in the means between computer generated sequences and human generated sequences are count.max.duplicates (F = 19.199, p = 1.909e-5) and num.consecutive (F = 15.452, p = 1.1169e-4). Human generated sequences have significantly different highest-frequency of digits and number of consecutive digits compared to computer generated sequences.</p>
<pre class="r"><code>features %&gt;%
  mutate(&quot;Source&quot; = if_else(Is.Human == 1, &quot;Human&quot;, &quot;Computer&quot;)) %&gt;%
  select(Source, count.max.duplicates, num.consecutive) %&gt;%
  pivot_longer(c(count.max.duplicates, num.consecutive), names_to=&quot;var&quot;, values_to=&quot;value&quot;) %&gt;%
  ggplot(aes(Source,value,fill=Source)) +
  scale_x_discrete(labels=c(&quot;Human&quot;,&quot;Computer&quot;))+
  geom_bar(stat=&quot;summary&quot;) + 
  geom_errorbar(stat=&quot;summary&quot;) +
  labs(title = &quot;Means of significant variables&quot;) +
  facet_wrap(~var, nrow=2) +
  coord_flip() +
  ylab(&quot;&quot;) +
  theme(legend.position = &quot;none&quot;)</code></pre>
<p><img src="/project/internal-project/index_files/figure-html/unnamed-chunk-16-1.png" width="768" style="display: block; margin: auto;" /></p>
</div>
<div id="assumptions" class="section level3">
<h3>Assumptions</h3>
<p>The assumption of random and independent observations was likely not very well met. The people surveyed to generate random numbers were not a random sample they were all people from my friend group and family. Additionally, each person surveyed generated multiple sequences. Since it is very possible that the features of the sequences generated by humans vary from person to person, the assumption of independent observations was not really met (I could test if there are any obviously significant differences between the features of the sequences generated by different people to explore this more, but I’m not going to).</p>
</div>
</div>
<div id="randomization-test" class="section level1">
<h1>Randomization Test</h1>
<p>To again test if together the distributions of count.max.duplicates and num.consecutive are significantly different for sequences of numbers generated by humans and by computers, I will perform a PERMANOVA test. This test works by calculating a base F-statistic that describes the correlation of the explanatory variables with the response variables, and then randomly scrambling the order of the explanatory variables to break any existing correlation a large number of times and creating an F statistic from the scrambled data. The number of scrambled-F statistics that are more extreme than the original F statistic can be used to calculate a P-statistic.</p>
<p>H<sub>0</sub>: The mean and spread of count.max.duplicates and num.consecutive do not differ between sequences created by humans and sequences created by a computer.</p>
<p>H<sub>A</sub>: The mean or spread of count.max.duplicates or num.consecutive differ between sequences created by humans and sequences created by a computer.</p>
<pre class="r"><code>dists &lt;- features %&gt;%
  select(count.max.duplicates, num.consecutive) %&gt;%
  dist()
adonis(dists~Is.Human,data=features, permutations = 9999)</code></pre>
<pre><code>## 
## Call:
## adonis(formula = dists ~ Is.Human, data = features, permutations = 9999) 
## 
## Permutation: free
## Number of permutations: 9999
## 
## Terms added sequentially (first to last)
## 
##            Df SumsOfSqs MeanSqs F.Model      R2 Pr(&gt;F)    
## Is.Human    1      34.6  34.600  16.108 0.07523  1e-04 ***
## Residuals 198     425.3   2.148         0.92477           
## Total     199     459.9                 1.00000           
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The p-value of 1e-4 is smaller than the significance level of 0.10, so we may reject the null hypothesis and conclude that there is actually a difference between the distributions of these two variables in different source groups.</p>
<p>A plot showing the location of the test statistic on the dist distribution is shown bellow.</p>
<pre class="r"><code>ggplot(data.frame(&quot;dists&quot; = as.numeric(dists)), aes(x=dists)) +
  geom_histogram(binwidth = 0.4, boundary = 0) +
  geom_vline(xintercept = quantile(dists, 1e-04))</code></pre>
<p><img src="/project/internal-project/index_files/figure-html/unnamed-chunk-18-1.png" width="768" style="display: block; margin: auto;" /></p>
</div>
<div id="linear-regression" class="section level1">
<h1>Linear Regression</h1>
<p>We will now perform a linear regression predicting Is.Human from count.max.duplicates and num.consecutive.</p>
<div id="running-linear-regression-with-interaction" class="section level3">
<h3>Running Linear Regression (with interaction)</h3>
<p>The first step is to create mean centered versions of these variables.</p>
<pre class="r"><code>features &lt;- features %&gt;%
  mutate(&quot;count.max.duplicates_c&quot; = count.max.duplicates - mean(count.max.duplicates),
         &quot;num.consecutive_c&quot; = num.consecutive - mean(num.consecutive))</code></pre>
<p>Now we can create a linear model.</p>
<pre class="r"><code>model &lt;- lm(Is.Human ~ count.max.duplicates_c * num.consecutive_c, data = features)
summary(model)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Is.Human ~ count.max.duplicates_c * num.consecutive_c, 
##     data = features)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.77651 -0.38664 -0.01848  0.39944  1.12937 
## 
## Coefficients:
##                                            Estimate Std. Error t value
## (Intercept)                               5.000e-01  3.313e-02  15.091
## count.max.duplicates_c                   -2.139e-01  5.189e-02  -4.123
## num.consecutive_c                         8.801e-02  2.415e-02   3.644
## count.max.duplicates_c:num.consecutive_c  6.333e-05  3.472e-02   0.002
##                                          Pr(&gt;|t|)    
## (Intercept)                               &lt; 2e-16 ***
## count.max.duplicates_c                   5.53e-05 ***
## num.consecutive_c                        0.000343 ***
## count.max.duplicates_c:num.consecutive_c 0.998547    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.4666 on 196 degrees of freedom
## Multiple R-squared:  0.1464, Adjusted R-squared:  0.1333 
## F-statistic: 11.21 on 3 and 196 DF,  p-value: 8.074e-07</code></pre>
<p>The intercept is the average value of the Is.Human variable (1 for human, 0 for computer). This is 0.5 since there are the same number of human generated sequences as computer generated sequences. The coefficient for the max number of duplicates is -0.21. This means that for every additional duplicate greater than the mean, the predicted probability that it was created by a human is decreased by 21%. The coefficient for the number of consecutive numbers is 0.088. This means that for every additional consecutive number above the mean, the predicted probability that it was created by a human increases by 8.8%. The coefficient for the interaction between the two variables is 6.33e-5. This means that for every increase of value in the number of duplicates, the coefficient of the number of consecutive digits is increased by 6.33e-5 (this is a super small value and is not significantly different from 0, so interaction can be ignored going forward).</p>
<pre class="r"><code>line_x &lt;- seq(from = 1.6, to = 5.3, length.out = 200)
line &lt;- (-1 * (line_x - 
                  mean(features$count.max.duplicates)) * -2.139e-01) / 
        8.801e-02 + mean(features$num.consecutive)

ggplot(features, aes(x = count.max.duplicates, y = num.consecutive)) +
  #scale_x_continuous(limits=c(1.7,5.3)) + 
  #scale_y_continuous(limits=c(-0.3,7.3)) + 
  geom_ribbon(aes(x = line_x, ymax= if_else(line &lt; 7.3, line, 7.3), 
                  ymin=-0.4, alpha = 0.2, fill = &quot;Computer&quot;)) +
  geom_ribbon(aes(x = line_x, ymax= 7.3, ymin=if_else(line &lt; 7.3, line, 7.3), 
                  alpha = 0.2, fill = &quot;Human&quot;))+
  geom_jitter(width = 0.35, height = 0.35,
              aes(color = if_else(Is.Human == 1, &quot;Human&quot;, &quot;Computer&quot;))) +
  geom_point(aes(color = if_else(Is.Human == 1, &quot;Human&quot;, &quot;Computer&quot;))) +
  labs(color = &quot;Source&quot;, fill = &quot;Predicted as&quot;, title = &quot;Regression of Significant variables&quot;) +
  xlab(&quot;Maximum number of duplicate digits&quot;) +
  ylab(&quot;Number of consecutive digits&quot;) +
  scale_alpha(guide = &quot;none&quot;)</code></pre>
<p><img src="/project/internal-project/index_files/figure-html/unnamed-chunk-21-1.png" width="768" style="display: block; margin: auto;" /></p>
</div>
<div id="checking-assumptions" class="section level3">
<h3>Checking Assumptions</h3>
<p>It is likely the assumption of linearity fails as there does not appear to be a very strong linear relationship. Additionally, the scatterplot does not seem to be very normally distributed as the values for maximum number of duplicate digits are grouped on the left.</p>
</div>
<div id="testing-with-robust-standard-errors" class="section level3">
<h3>Testing with Robust Standard Errors</h3>
<p>We can use the coeftest method to perform a test on the linear regression model to get more cautious p-values.</p>
<pre class="r"><code>coeftest(model, vcov=vcovHC(model))</code></pre>
<pre><code>## 
## t test of coefficients:
## 
##                                             Estimate  Std. Error t value
## (Intercept)                               5.0001e-01  3.4904e-02 14.3253
## count.max.duplicates_c                   -2.1391e-01  6.8146e-02 -3.1390
## num.consecutive_c                         8.8011e-02  2.6788e-02  3.2854
## count.max.duplicates_c:num.consecutive_c  6.3328e-05  5.3216e-02  0.0012
##                                           Pr(&gt;|t|)    
## (Intercept)                              &lt; 2.2e-16 ***
## count.max.duplicates_c                    0.001957 ** 
## num.consecutive_c                         0.001206 ** 
## count.max.duplicates_c:num.consecutive_c  0.999052    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>With this additional test, nothing changed; the two variables are still significantly correlated with the source of the sequence (p-values of 1.96e-3 and 1.21e-3) and the interaction is still insignificant with p-value 0.999.</p>
</div>
<div id="effect-size" class="section level3">
<h3>Effect Size</h3>
<p>With an R<sup>2</sup> value of 0.146, the linear regression model explains 14.6% of the variation in the source by the maximum number of duplicates in a sequence and the number of consecutive digits in a sequence.</p>
</div>
<div id="linear-regression-without-interaction" class="section level3">
<h3>Linear Regression Without Interaction</h3>
<p>We can now run the same linear regression without interactions to see the effect of the interactions.</p>
<pre class="r"><code>model_meffects &lt;- lm(Is.Human ~ count.max.duplicates_c + num.consecutive_c, data = features)
summary(model_meffects)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Is.Human ~ count.max.duplicates_c + num.consecutive_c, 
##     data = features)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.77657 -0.38664 -0.01865  0.39945  1.12919 
## 
## Coefficients:
##                        Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)             0.50000    0.03291  15.192  &lt; 2e-16 ***
## count.max.duplicates_c -0.21391    0.05175  -4.133 5.29e-05 ***
## num.consecutive_c       0.08801    0.02405   3.659 0.000325 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.4655 on 197 degrees of freedom
## Multiple R-squared:  0.1464, Adjusted R-squared:  0.1377 
## F-statistic:  16.9 on 2 and 197 DF,  p-value: 1.69e-07</code></pre>
<p>The coefficients and R<sup>2</sup> values of the model without interaction are almost identical to that of the model with interaction. This can be easily explained by the fact that the model including interaction showed an extremely small level of interaction that was about as insignificant as one can get. We can use a likelihood ratio test to formally show that the models are not statistically different.</p>
<pre class="r"><code>lrtest(model, model_meffects)</code></pre>
<pre><code>## Likelihood ratio test
## 
## Model 1: Is.Human ~ count.max.duplicates_c * num.consecutive_c
## Model 2: Is.Human ~ count.max.duplicates_c + num.consecutive_c
##   #Df  LogLik Df Chisq Pr(&gt;Chisq)
## 1   5 -129.33                    
## 2   4 -129.33 -1     0     0.9985</code></pre>
<p>With a p-value of 0.9985, there is no significant difference between the two models.</p>
</div>
</div>
<div id="logistic-regression" class="section level1">
<h1>Logistic Regression</h1>
<p>Wee will now run a logistic regression predicting the source of the random number sequences from the maximum number of duplicate digits and the number of consecutive digits. We can reuse the mean centered variables created when running the linear regression test.</p>
<pre class="r"><code>model &lt;- glm(Is.Human ~ count.max.duplicates_c + num.consecutive_c, 
             data = features, family = &quot;binomial&quot;)
summary(model)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Is.Human ~ count.max.duplicates_c + num.consecutive_c, 
##     family = &quot;binomial&quot;, data = features)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -1.76324  -0.96046  -0.01944   0.98549   2.45624  
## 
## Coefficients:
##                         Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)            -0.000442   0.153774  -0.003 0.997707    
## count.max.duplicates_c -1.004141   0.259436  -3.870 0.000109 ***
## num.consecutive_c       0.423695   0.123276   3.437 0.000588 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 277.26  on 199  degrees of freedom
## Residual deviance: 245.40  on 197  degrees of freedom
## AIC: 251.4
## 
## Number of Fisher Scoring iterations: 3</code></pre>
<p>The coefficients represent the change in the predicted log-odds of the source being human for a 1 unit change in the measurement. The intercept is -4.42e-4. This corresponds to an odd of 1, or a probability of 0.5 of the observation being generated by a human when the other variables are equal to their means. The coefficient of count.max.duplicates_c is -1.00. This means that for every increase of 1 in count.max.duplicates_c, the predicted log-odds of the source being human will decrease by 1.00. The coefficient of num.consecutive_c is 0.424. This means that for every increase in num.consecutive_c of 1, the predicted log-odds of the source being human will increase by 0.424.</p>
<div id="quality-of-the-logistic-regression-model" class="section level3">
<h3>Quality of the Logistic Regression Model</h3>
<p>We can create a confusion matrix and generate some other relevant statistics using the confusionMatrix function of the caret library (we use <code>{R} positive=&quot;Human&quot;</code> to make a prediction of Human the positive .</p>
<pre class="r"><code>truth &lt;- factor(if_else(features$Is.Human == 1, &quot;Human&quot;, &quot;Computer&quot;))
pred &lt;- factor(if_else(as.numeric(predict(model, type = &quot;response&quot;)) &gt; 0.5, &quot;Human&quot;, &quot;Computer&quot;))
confusionMatrix(truth, pred, positive = &quot;Human&quot;)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction Computer Human
##   Computer       62    38
##   Human          32    68
##                                           
##                Accuracy : 0.65            
##                  95% CI : (0.5795, 0.7159)
##     No Information Rate : 0.53            
##     P-Value [Acc &gt; NIR] : 0.0003917       
##                                           
##                   Kappa : 0.3             
##                                           
##  Mcnemar&#39;s Test P-Value : 0.5500973       
##                                           
##             Sensitivity : 0.6415          
##             Specificity : 0.6596          
##          Pos Pred Value : 0.6800          
##          Neg Pred Value : 0.6200          
##              Prevalence : 0.5300          
##          Detection Rate : 0.3400          
##    Detection Prevalence : 0.5000          
##       Balanced Accuracy : 0.6505          
##                                           
##        &#39;Positive&#39; Class : Human           
## </code></pre>
<p>The accuracy of the model is 0.65: 65% of the observations were predicted correctly. The sensitivity is 0.642: 64.2% of the human generated sequences were correctly identified as such. The specificity is 0.660: 66.0% of the computer generated sequences were correctly identified as such. The recall (positive predicted value) is 0.680: 68.0% of sequences predicted as human generated actually were human generated.</p>
</div>
<div id="density-plot" class="section level3">
<h3>Density Plot</h3>
<pre class="r"><code>logit&lt;-predict(model) #get predicted log-odds
truth&lt;-factor(if_else(features$Is.Human == 1, &quot;Human&quot;, &quot;Computer&quot;))
ggplot(features,aes(x=logit, fill=truth)) +
  geom_density(alpha=.3) +
  geom_vline(xintercept=0, lty=2) +
  labs(title = &quot;Density Plot of Logistic Regression&quot;, fill = &quot;Source&quot;) +
  xlab(&quot;Predicted Log-odds&quot;) +
  ylab(&quot;Density&quot;)</code></pre>
<p><img src="/project/internal-project/index_files/figure-html/unnamed-chunk-27-1.png" width="768" style="display: block; margin: auto;" /></p>
<p>There is a lot of overlap suggesting that the final model created to classify the source will not be very good. It does imply, however, that we’ll be able to do a little bit better than random guessing since there are portions of the graph that do not overlap.</p>
</div>
<div id="roc-and-auc" class="section level3">
<h3>ROC and AUC</h3>
<p>We can now calculate the ROC plot of the logistic model.</p>
<pre class="r"><code>ROCplot &lt;- ggplot(features) +
  geom_roc(aes(d=Is.Human, m=predict(model, type = &quot;response&quot;)), n.cuts=0) +
  labs(title = &quot;ROC Plot of Logistic Regression Model&quot;)
ROCplot</code></pre>
<p><img src="/project/internal-project/index_files/figure-html/unnamed-chunk-28-1.png" width="768" style="display: block; margin: auto;" /></p>
<p>The ROC curve is concave down, which is a good sign for the accuracy of the model.</p>
<p>We can now calculate AUC, the area under the ROC curve.</p>
<pre class="r"><code>calc_auc(ROCplot)$AUC</code></pre>
<pre><code>## [1] 0.7232</code></pre>
<p>The AUC is 0.723. This gives the model a fair classification for its ability to be both sensitive and specific.</p>
</div>
<div id="cross-validation" class="section level3">
<h3>Cross Validation</h3>
<p>We can perform a 10 Fold Cross Validation to determine the out of sample statistics for the model. The dataset was already randomized at the beginning of the project, so this does not need to be done again. The function “diags” described in class will be used to summarize the data.</p>
<pre class="r"><code>set.seed(666)
k = 10

folds&lt;-cut(seq(1:nrow(features)),breaks=k,labels=F)

diags&lt;-NULL
for(i in 1:k){ 
  train&lt;-features[folds!=i,]
  test&lt;-features[folds==i,]
  truth&lt;-test$Is.Human
  fit&lt;- glm(Is.Human ~ count.max.duplicates_c + num.consecutive_c,
            data = train, family=&quot;binomial&quot;)
  probs&lt;- predict(fit, newdata = test, type = &quot;response&quot;)
  diags&lt;-rbind(diags,class_diag(probs,truth))
}

apply(diags,2,mean)</code></pre>
<pre><code>##       acc      sens      spec       ppv       auc 
## 0.6200000 0.6283134 0.6417166 0.6620377 0.7073150</code></pre>
<p>The out-of-sample accuracy, sensitivity, and recall (ppv) are 0.620, 0.628, and 0.662. The AUC is 0.707, which is only slightly less than the in-sample AUC, suggesting there was very little overfitting (most likely due to the extensive analysis of which variables were actual predictors before selecting the variables to use in the model).</p>
</div>
</div>
<div id="lasso-regression" class="section level1">
<h1>Lasso Regression</h1>
<p>We will now perform a Lasso regression using every feature gathered about the data.</p>
<pre class="r"><code>y&lt;-as.matrix(features$Is.Human)  ###save response variable 
x&lt;-features %&gt;% select(count.0, count.1, count.2, count.3,
                       count.4, count.5, count.6, count.7,
                       count.8, count.9, count.max.duplicates,
                       num.consecutive, derivative,
                       mean, std.dev, num.odd, num.even) %&gt;%
  scale %&gt;%
  as.matrix

cv&lt;-cv.glmnet(x,y,family=&quot;binomial&quot;)
lasso&lt;-glmnet(x,y,family=&quot;binomial&quot;,lambda=cv$lambda.1se)
coef(lasso)</code></pre>
<pre><code>## 18 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                                 s0
## (Intercept)          -0.0003727751
## count.0               .           
## count.1               .           
## count.2               .           
## count.3               .           
## count.4               .           
## count.5               .           
## count.6               .           
## count.7               .           
## count.8               .           
## count.9               .           
## count.max.duplicates -0.2112269656
## num.consecutive       0.1474339995
## derivative            .           
## mean                  .           
## std.dev               .           
## num.odd               .           
## num.even              .</code></pre>
<p>Interestingly enough the only predictors with non 0 coefficients are the two predictors we determined to be significant in the previous sections of this project: count.max.duplicates and num.consecutive. For this reason, the 10-fold CV will be exactly the same as the 10-Fold CV in the previous section, and will be excluded.</p>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
<p>I found two features of a randomly generated numbers that are significant predictors of whether the sequence of numbers was generated by a human or computer. These two features are the maximum number of times any one digit is repeated in a sequence and the number of consecutive numbers in the digits of a sequence. These two features describe 14.6% of the variation in the source of the sequence, and together can be used to create a logistic model with a 62.0% accuracy and AUC of 0.707. This can be described as a model of fair quality, and is definitely better than guessing.</p>
</div>
